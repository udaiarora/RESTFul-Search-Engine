<HTML>
<HEAD>
<TITLE>Criteria for Optimality</TITLE>
<LINK REL="STYLESHEET" TYPE="text/css" HREF="../sas.css">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<A NAME="nlpcfo">&#13;</A>
<!--Navigation Panel-->
<TABLE BORDER="0" CELLPADDING="0">
<TR VALIGN="TOP">
  <TD ALIGN="CENTER">
  <A NAME="topofpage" HREF="index.htm">
  <IMG BORDER="0" SRC="../../common/images/cont1.gif" ALT="Chapter Contents" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Chapter Contents</FONT></A></TD>
  <TD ALIGN=CENTER>
  <A HREF="sect25.htm"><IMG BORDER="0" SRC="../../common/images/prev1.gif" ALT="Previous" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Previous</FONT></A></TD>
  <TD ALIGN=CENTER>
  <A HREF="sect27.htm"><IMG BORDER="0" SRC="../../common/images/next1.gif" ALT="Next" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Next</FONT></A></TD>
</TR>
</TABLE>
<TABLE BGCOLOR="#CCCC99" WIDTH="100%" CELLPADDING=4>
<TR>
  <TD VALIGN=MIDDLE CLASS="chaphead"><I><FONT SIZE="2">The NLP Procedure</FONT></I></TD>
</TR>
</TABLE><BR>
<P><!--End of Navigation Panel-->
<H2>Criteria for Optimality</H2>
<P>PROC NLP solves
<P>
<DL CLASS="equation"><DD><IMG WIDTH="271" HEIGHT="123"
 SRC="images/nlpeq76.gif"
 ALT="& \min f(x) , & x \in {\cal R}^n \ {s.t.} & c_i(x) = 0 , & i = 1, ... ,m_e \ & c_i(x) \ge 0 , & i = m_e+1, ... ,m 
 "></DL>
<P>where <SPAN CLASS="mathfont"><I>f</I></SPAN> is the objective function and the <SPAN CLASS="mathfont"><I>m</I></SPAN>
<SPAN CLASS="mathfont"><I>c</I><SUB><I>i</I></SUB></SPAN>'s are the constraint functions.
<P><A NAME="idxnlp0378">&#13;</A><A NAME="idxnlp0379">&#13;</A><A NAME="idxnlp0380">&#13;</A><A NAME="idxnlp0381">&#13;</A><A NAME="idxnlp0382">&#13;</A><A NAME="idxnlp0383">&#13;</A>A point x is feasible if it satisfies all the constraints.  
The feasible region <SPAN CLASS="mathfont"><I>G</I></SPAN> is the set of all the feasible points.
<SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> is a global solution of the preceeding problem if no point in <SPAN CLASS="mathfont"><I>G</I></SPAN> 
has a lower function value than f(<SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN>).  
<SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> is a local solution of the problem if there exists some
open neighborhood surrounding <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> in that no point has a lower 
function value than f(<SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN>). 
Nonlinear Programming algorithms cannot consistently find  
global minima.  All the algorithms in PROC NLP find a 
local minimum for this problem.  If you need to check whether the obtained solution
is a global minimum, you may have to run PROC NLP with 
different starting points obtained either at random or by selecting
a point on a grid that contains <SPAN CLASS="mathfont"><I>G</I></SPAN>.
<P>The local minimizer <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> of this problem
satisfies the following local optimality conditions:
<UL>
<LI> The grdient (vector of first derivatives) <IMG WIDTH="128" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="images/nlpeq77.gif"
 ALT="g(x^*) = \nabla f(x^*)">      of the objective function <SPAN CLASS="mathfont"><I>f</I></SPAN> (projected toward the
      feasible region if the problem is constrained) at the point <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> is zero.
<LI> The Hessian (matrix of second derivatives)  <IMG WIDTH="141" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="images/nlpeq78.gif"
 ALT="G(x^*) = \nabla^2 f(x^*)">      of the objective function <SPAN CLASS="mathfont"><I>f</I></SPAN> (projected
      toward the feasible region <SPAN CLASS="mathfont"><I>G</I></SPAN> in the constrained case) 
      at the point <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> is positive definite.
<P></UL>
<P>Most of the optimization algorithms in PROC NLP use iterative techniques
that result in a sequence of points <SPAN CLASS="mathfont"><I>x<SUP>0</SUP></I>,...,<I>x</I><SUP><I>n</I></SUP>,...</SPAN>, that
converges to a local solution <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN>.  At the solution, PROC NLP 
performs tests to confirm that the (projected) gradient is close to 
zero and that the (projected) Hessian matrix is positive definite.
<P><H4><I>Karush-Kuhn-Tucker Conditions</I></H4>
<P>An important tool in the analysis and design of algorithms in constrained
optimization is the <I>Lagrangian Function</I>, that is a linear combination of 
the objective function and the constraints:
<P>
<DL CLASS="equation"><DD><IMG WIDTH="219" HEIGHT="110"
 SRC="images/nlpeq79.gif"
 ALT="L(x,\lambda) = f(x) - \sum_{i=1}^m \lambda_i c_i(x) "></DL>
<P>The coefficients <IMG WIDTH="20" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="images/nlpeq80.gif"
 ALT="\lambda_i"> are called <I>Lagrange multipliers</I>.
This tool makes it possible to state necessary and sufficient 
conditions for a local minimum.  The various algorithms in PROC NLP 
create sequences of points, each of that is closer than the previous 
one to satisfying these conditions.
<P>Assuming that the functions <SPAN CLASS="mathfont"><I>f</I></SPAN> and <SPAN CLASS="mathfont"><I>c</I><SUB><I>i</I></SUB></SPAN> are twice continuously
differentiable, the point <SPAN CLASS="mathfont"><I>x<SUP>*</SUP></I></SPAN> is a <EM>local
minimum</EM> of the nonlinear programming problem, if there exists a vector  
<IMG WIDTH="143" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="images/nlpeq81.gif"
 ALT="\lambda^*=(\lambda_1^*, ... ,\lambda_m^*)"> that meets the following
conditions.
<P><A NAME="idxnlp0384">&#13;</A><A NAME="idxnlp0385">&#13;</A><B>1. first-order, Karush-Kuhn-Tucker conditions:</B>

<DL CLASS="equation"><DD><IMG WIDTH="477" HEIGHT="123"
 SRC="images/nlpeq82.gif"
 ALT="c_i(x^*) = 0 , & &
 & i = 1,  ...  ,m_e, \ c_i(x^*) \ge 0 , & \lambda_i^* \ge 0,...
 ...^* c_i(x^*) = 0 ,
 & i = m_e+1,  ...  ,m \ \nabla_x L(x^*,\lambda^*) = 0 & & &
 "></DL>
<P><B>2. Second-order conditions:</B>
<P>Each nonzero vector <IMG WIDTH="60" HEIGHT="30" ALIGN="MIDDLE" BORDER="0"
 SRC="images/nlpeq83.gif"
 ALT="y \in {\cal R}^n"> for which

<DL CLASS="equation"><DD><IMG WIDTH="381" HEIGHT="101"
 SRC="images/nlpeq84.gif"
 ALT="y^T \nabla_x c_i(x^*) = 0 
 \{ i = 1, ... ,m_e , \ \forall i\in \{ m_e+1, ... ,m: \lambda_i^* \gt 0 \}
 ."></DL>
satisfies

<DL CLASS="equation"><DD><IMG WIDTH="157" HEIGHT="77"
 SRC="images/nlpeq85.gif"
 ALT="y^T \nabla_x^2 L(x^*,\lambda^*) y \gt 0 "></DL>
<P>Most of the algorithms to solve this problem attempt to find a 
combination of vectors x and <IMG WIDTH="14" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="images/nlpeq86.gif"
 ALT="\lambda"> for which the gradient  
of the Langrangian function in respect to x is zero.
<P><H3><I><A NAME="nlpd">Derivatives</A></I></H3>
<A NAME="idxnlp0386">&#13;</A><A NAME="idxnlp0387">&#13;</A><A NAME="idxnlp0388">&#13;</A><A NAME="idxnlp0389">&#13;</A>The first and second order conditions of optimality are based
on first and second derivates of the object function <SPAN CLASS="mathfont"><I>f</I></SPAN> and the
constraints <SPAN CLASS="mathfont"><I>c</I><SUB><I>i</I></SUB></SPAN>.
<P>The gradient<A NAME="idxnlp0390">&#13;</A>vector contains the first
derivatives of the objective function <SPAN CLASS="mathfont"><I>f</I></SPAN> with respect
to the parameters <SPAN CLASS="mathfont"><I>x<SUb>1</SUb></I>, ... ,<I>x</I><sub><I>n</I></sub>,</SPAN> as follows:

<DL CLASS="equation"><DD><IMG WIDTH="177" HEIGHT="98"
 SRC="images/nlpeq87.gif"
 ALT="g(x) = \nabla f(x) = (\frac{\partial f}{\partial x_j}) "></DL>
<P>The <SPAN CLASS="mathfont"><I>n</I> &times;<I>n</I></SPAN> symmetric Hessian matrix<A NAME="idxnlp0391">&#13;</A>contains
the second derivatives of the objective function <SPAN CLASS="mathfont"><I>f</I></SPAN> with
respect to the parameters <SPAN CLASS="mathfont"><I>x<SUb>1</SUb></I>, ... ,<I>x</I><sub><I>n</I></sub>,</SPAN> as follows:

<DL CLASS="equation"><DD><IMG WIDTH="224" HEIGHT="101"
 SRC="images/nlpeq88.gif"
 ALT="G(x) = \nabla^2 f(x) = (\frac{\partial^2 f}{\partial x_j \partial x_k}) ."></DL>
<P>For Least-Squares problems, the <SPAN CLASS="mathfont"><I>m</I> &times;<I>n</I></SPAN> Jacobian 
matrix<A NAME="idxnlp0392">&#13;</A>contains the first-order derivatives of <SPAN CLASS="mathfont"><I>m</I></SPAN> objective
functions <SPAN CLASS="mathfont"><I>f</I><SUB><I>i</I></SUB>(<I>x</I>)</SPAN> with respect to the parameters
<SPAN CLASS="mathfont"><I>x<SUb>1</SUb></I>, ... ,<I>x</I><sub><I>n</I></sub>,</SPAN> as follows:

<DL CLASS="equation"><DD><IMG WIDTH="253" HEIGHT="98"
 SRC="images/nlpeq89.gif"
 ALT="J(x) = (\nabla f_1, ... ,\nabla f_m) =
 (\frac{\partial f_i}{\partial x_j}) "></DL>
In case of Least-Squares problems, the crossproduct Jacobian <SPAN CLASS="mathfont"><I>J</I><SUP><I>T</I></SUP><I>J</I></SPAN>,

<DL CLASS="equation"><DD><IMG WIDTH="166" HEIGHT="110"
 SRC="images/nlpeq90.gif"
 ALT="J^TJ = ({\sum_{i=1}^m \frac{\partial f_i}{\partial x_j}
 \frac{\partial f_i}{\partial x_k}})"></DL>
is used as an approximate Hessian matrix. It is a very good
approximation of the Hessian if the residuals at the solution are &#34;small.&#34;
(If the residuals are not sufficiently small at the solution, this approach 
may result in slow convergence.)
The fact that it is possible to obtain Hessian approximations 
for this problem that do not require any computation of second
derivatives means that Least-Squares algorithms are more efficient
than unconstrained optimization algorithms.  Using the vector
<SPAN CLASS="mathfont"><I>f</I> = <I>f</I>(<I>x</I>)</SPAN> of function values,  <SPAN CLASS="mathfont"><I>f</I>(<I>x</I>) = (<I>f<SUb>1</SUb></I>(<I>x</I>), ... ,<I>f</I><sub><I>m</I></sub>(<I>x</I>))<sup><I>T</I></sup></SPAN>,
PROC NLP computes the gradient <SPAN CLASS="mathfont"><I>g</I>=<I>g</I>(<I>x</I>)</SPAN> by 

<DL CLASS="equation"><DD>
<SPAN CLASS="mathfont"><I>g</I>(<I>x</I>) = <I>J</I><SUP><I>T</I></SUP>(<I>x</I>) <I>f</I>(<I>x</I>) 
</SPAN>
</DL>
<P>The <SPAN CLASS="mathfont"><I>mc</I> &times;<I>n</I></SPAN> Jacobian matrix contains the first-order
derivatives of <SPAN CLASS="mathfont"><I>mc</I></SPAN> nonlinear constraint functions <SPAN CLASS="mathfont"><I>c</I><SUB><I>i</I></SUB>(<I>x</I>),</SPAN>
<SPAN CLASS="mathfont"><I>i</I> = 1, ... ,<I>mc</I>,</SPAN> with respect to the parameters
<SPAN CLASS="mathfont"><I>x<SUb>1</SUb></I>, ... ,<I>x</I><sub><I>n</I></sub>,</SPAN> as follows:

<DL CLASS="equation"><DD><IMG WIDTH="270" HEIGHT="98"
 SRC="images/nlpeq91.gif"
 ALT="CJ(x) = (\nabla c_1, ... ,\nabla c_{mc}) =
 (\frac{\partial c_i}{\partial x_j}) "></DL>
<P>PROC NLP provides three ways to compute derivatives:
<UL>
<LI> It computes analytical first- and second-order derivatives
      of the objective function <SPAN CLASS="mathfont"><I>f</I></SPAN> with respect to
      the <SPAN CLASS="mathfont"><I>n</I></SPAN> variables <SPAN CLASS="mathfont"><I>x</I><SUB><I>j</I></SUB></SPAN>.
<LI> It computes first- and second-order finite difference
      approximations to the derivatives. For more information,
      see the section <A HREF="sect28.htm#nlpfdad">&#34;Finite-Difference Approximations of Derivatives&#34;</A>.
<LI> The user supplies formulas for analytical or
      numerical first- and second-order derivatives of the
      objective function in the GRADIENT, JACOBIAN, CRPJAC,
      and HESSIAN statements. The JACNLC statement can be used
      to specify the derivatives for the nonlinear constraints.
</UL><P>
<!--Navigation Panel-->
<TABLE BORDER="0" CELLPADDING="0">
<TR VALIGN="TOP">
  <TD ALIGN="CENTER">
  <A HREF="index.htm">
  <IMG BORDER="0" SRC="../../common/images/cont1.gif" ALT="Chapter Contents" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Chapter Contents</FONT></A></TD>
  <TD ALIGN=CENTER>
  <A HREF="sect25.htm"><IMG BORDER="0" SRC="../../common/images/prev1.gif" ALT="Previous" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Previous</FONT></A></TD>
  <TD ALIGN=CENTER>
  <A HREF="sect27.htm"><IMG BORDER="0" SRC="../../common/images/next1.gif" ALT="Next" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Next</FONT></A></TD>
  <TD ALIGN=CENTER>
  <A HREF="#topofpage">
  <IMG BORDER="0" SRC="../../common/images/top1.gif" ALT="Top" WIDTH="99" HEIGHT="16"><BR><FONT SIZE="-2">Top</FONT></A></TD>
</TR>
</TABLE>
<P><!--End of Navigation Panel-->
<P><FONT SIZE="1"><A HREF="../../common/images/copyrite.htm">Copyright &copy; 1999 by SAS Institute Inc., Cary, NC, USA. All rights reserved.</A></FONT>
</BODY>
</HTML>
